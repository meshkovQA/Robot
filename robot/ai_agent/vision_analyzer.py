# robot/ai_agent/vision_analyzer.py

import json
import logging
from openai import OpenAI
import os
import cv2
import base64
from datetime import datetime
from pathlib import Path


class VisionAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —Ä–æ–±–æ—Ç–∞
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º SimpleAIDetector –∏ OpenAI GPT-4V
    """

    def __init__(self, config, camera=None, ai_detector=None):
        self.config = config
        self.camera = camera
        self.ai_detector = ai_detector

        # –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á —Ç–æ–ª—å–∫–æ –∏–∑ environment –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        self.api_key = os.getenv('OPENAI_API_KEY')

        if not self.api_key:
            raise ValueError(
                "OpenAI API key –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY")

        # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç —Å –Ω–æ–≤—ã–º API
        self.client = OpenAI(api_key=self.api_key)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è GPT-4V (–æ—Å—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å)
        self.vision_model = config.get('vision_model', 'gpt-4o-mini')
        self.max_tokens = config.get('vision_max_tokens', 300)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (–æ—Å—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å)
        if not self.ai_detector:
            logging.warning(
                "‚ö†Ô∏è SimpleAIDetector –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω - –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        else:
            logging.info(
                "‚úÖ VisionAnalyzer –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π SimpleAIDetector")

        if not self.camera:
            logging.warning("‚ö†Ô∏è –ö–∞–º–µ—Ä–∞ –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞")
        else:
            logging.info("‚úÖ VisionAnalyzer –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ –∫–∞–º–µ—Ä–µ")

        logging.info("üëÅÔ∏è VisionAnalyzer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –Ω–æ–≤—ã–º OpenAI API")

    def capture_frame(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã"""
        if not self.camera:
            logging.error("‚ùå –ö–∞–º–µ—Ä–∞ –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞")
            return None

        try:
            # –ü–æ–ª—É—á–∞–µ–º –∫–∞–¥—Ä –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º—ã –∫–∞–º–µ—Ä—ã
            frame = self.camera.get_frame()
            if frame is not None:
                logging.debug("üì∑ –ö–∞–¥—Ä –ø–æ–ª—É—á–µ–Ω —Å –∫–∞–º–µ—Ä—ã")
                return frame
            else:
                logging.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã")
                return None
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞—Ö–≤–∞—Ç–∞ –∫–∞–¥—Ä–∞: {e}")
            return None

    def detect_objects(self, frame):
        """–î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π SimpleAIDetector"""
        if not self.ai_detector:
            logging.warning("‚ö†Ô∏è AI –¥–µ—Ç–µ–∫—Ç–æ—Ä –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω")
            return []

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ –¥–µ—Ç–µ–∫—Ü–∏–∏
            detections = self.ai_detector.detect_objects(frame)

            # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç –∫ –Ω–∞—à–µ–º—É API
            detected_objects = []
            for det in detections:
                detected_objects.append({
                    'class': det.get('class_name', 'unknown'),
                    'confidence': det.get('confidence', 0.0),
                    'bbox': det.get('bbox', [0, 0, 0, 0]),  # [x, y, w, h]
                    'center': det.get('center', [0, 0])
                })

            logging.info(f"üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {len(detected_objects)}")
            for obj in detected_objects:
                logging.debug(f"   {obj['class']} ({obj['confidence']:.2f})")

            return detected_objects

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤: {e}")
            return []

    def encode_image_to_base64(self, frame):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∫–∞–¥—Ä–∞ –≤ base64 –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ OpenAI"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ JPEG —Å —Ö–æ—Ä–æ—à–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º
            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 85]
            success, buffer = cv2.imencode('.jpg', frame, encode_param)

            if not success:
                logging.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ JPEG")
                return None

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64
            image_base64 = base64.b64encode(buffer).decode('utf-8')

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä (OpenAI –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)
            size_mb = len(image_base64) * 0.75 / 1024 / \
                1024  # –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤ MB
            if size_mb > 20:  # OpenAI –ª–∏–º–∏—Ç ~20MB
                logging.warning(
                    f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ: {size_mb:.1f}MB")
                # –ú–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∏–ª–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
                return None

            logging.debug(f"üì∑ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: {size_mb:.1f}MB")
            return image_base64

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return None

    def describe_scene_with_llm(self, frame, detected_objects=None):
        """–û–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã —á–µ—Ä–µ–∑ GPT-4V —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –≤–≤–æ–¥–æ–º"""
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–∞—Ö
            objects_text = ""
            if detected_objects:
                object_names = [obj['class'] for obj in detected_objects]
                objects_text = f"YOLO8 –æ–±–Ω–∞—Ä—É–∂–∏–ª –æ–±—ä–µ–∫—Ç—ã: {', '.join(object_names)}. "
            else:
                objects_text = "YOLO8 –Ω–µ –æ–±–Ω–∞—Ä—É–∂–∏–ª –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. "

            # –ö–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è OpenAI
            image_base64 = self.encode_image_to_base64(frame)

            if image_base64:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º GPT-4V —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": f"{objects_text}–û–ø–∏—à–∏ —á—Ç–æ —Ç—ã –≤–∏–¥–∏—à—å –Ω–∞ —ç—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, –∫–∞–∫ –µ—Å–ª–∏ –±—ã —Ç—ã —Ä–æ–±–æ—Ç-–ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ–∑—è–∏–Ω—É —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–æ–∫—Ä—É–≥. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º –∏ –ø–æ–Ω—è—Ç–Ω—ã–º."
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}",
                                    "detail": "low"  # –≠–∫–æ–Ω–æ–º–∏–º —Ç–æ–∫–µ–Ω—ã
                                }
                            }
                        ]
                    }
                ]

                logging.info("üß† –û—Ç–ø—Ä–∞–≤–ª—è—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ GPT-4V...")

                response = self.client.chat.completions.create(
                    model=self.vision_model,
                    messages=messages,
                    max_tokens=self.max_tokens,
                    temperature=0.3  # –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
                )

                description = response.choices[0].message.content.strip()
                logging.info(
                    f"‚úÖ GPT-4V –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–ª—É—á–µ–Ω–æ: '{description[:100]}...'")
                return description

            else:
                # Fallback - –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ YOLO –¥–∞–Ω–Ω—ã—Ö
                if detected_objects:
                    object_names = [obj['class'] for obj in detected_objects]
                    confidence_info = []
                    for obj in detected_objects:
                        if obj['confidence'] > 0.8:
                            confidence_info.append(
                                f"{obj['class']} (—É–≤–µ—Ä–µ–Ω–Ω–æ)")
                        elif obj['confidence'] > 0.5:
                            confidence_info.append(
                                f"{obj['class']} (–≤–µ—Ä–æ—è—Ç–Ω–æ)")
                        else:
                            confidence_info.append(
                                f"{obj['class']} (–≤–æ–∑–º–æ–∂–Ω–æ)")

                    return f"–Ø –≤–∏–∂—É: {', '.join(confidence_info)}. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."
                else:
                    return "–Ø —Å–º–æ—Ç—Ä—é –≤–æ–∫—Ä—É–≥, –Ω–æ –Ω–µ –º–æ–≥—É —á–µ—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –Ω–∞ —Ç–µ–∫—É—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏."

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø–∏—Å–∞–Ω–∏—è —Å—Ü–µ–Ω—ã —á–µ—Ä–µ–∑ LLM: {e}")

            # Fallback - –ø—Ä–æ—Å—Ç–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ YOLO
            if detected_objects:
                object_names = [obj['class'] for obj in detected_objects]
                return f"–ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –ò–ò –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä –≤–∏–¥–∏—Ç: {', '.join(object_names)}"
            else:
                return "–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

    def analyze_current_view(self):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–∞"""
        try:
            logging.info("üëÅÔ∏è –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–∞...")

            # 1. –ü–æ–ª—É—á–∞–µ–º –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã
            frame = self.capture_frame()
            if frame is None:
                return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∫–∞–º–µ—Ä—ã"}

            # 2. –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç—ã —á–µ—Ä–µ–∑ YOLO
            detected_objects = self.detect_objects(frame)

            # 3. –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –æ—Ç GPT-4V
            description = self.describe_scene_with_llm(frame, detected_objects)

            # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–¥—Ä –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            frame_path = Path(f"data/temp/vision_analysis_{timestamp}.jpg")
            frame_path.parent.mkdir(parents=True, exist_ok=True)

            try:
                cv2.imwrite(str(frame_path), frame)
                logging.debug(f"üíæ –ö–∞–¥—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {frame_path}")
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–¥—Ä: {e}")
                frame_path = None

            # 5. –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                "success": True,
                "description": description,
                "detected_objects": detected_objects,
                "objects_count": len(detected_objects),
                "frame_saved": str(frame_path) if frame_path else None,
                "timestamp": datetime.now().isoformat(),
                "analysis_method": "yolo + gpt4v" if self.api_key else "yolo_only"
            }

            logging.info(
                f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(detected_objects)} –æ–±—ä–µ–∫—Ç–æ–≤, –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–ª—É—á–µ–Ω–æ")
            return result

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–∞: {e}")
            return {
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def analyze_for_navigation(self, frame=None):
        """
        –ê–Ω–∞–ª–∏–∑ –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ - –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –∑–æ–Ω
        (–ë—É–¥—É—â–µ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è)
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –∫–∞–¥—Ä –∏–ª–∏ –ø–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–π
            if frame is None:
                frame = self.capture_frame()
                if frame is None:
                    return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏"}

            # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç—ã
            detected_objects = self.detect_objects(frame)

            # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç—ã –ø–æ —Ç–∏–ø–∞–º –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π
            obstacles = []
            people = []
            furniture = []

            for obj in detected_objects:
                obj_class = obj['class'].lower()

                if obj_class in ['person', 'people']:
                    people.append(obj)
                elif obj_class in ['chair', 'table', 'sofa', 'bed', 'desk', 'couch']:
                    furniture.append(obj)
                elif obj_class in ['dog', 'cat', 'bottle', 'bag', 'box']:
                    obstacles.append(obj)
                else:
                    obstacles.append(obj)  # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ –∫–∞–∫ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–µ

            # –û—Ü–µ–Ω–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è
            total_obstacles = len(obstacles) + len(furniture)
            has_people = len(people) > 0

            # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏
            if has_people:
                safety_level = "–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ"  # –õ—é–¥–∏ —Ä—è–¥–æ–º - –Ω—É–∂–Ω–∞ –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å
            elif total_obstacles > 3:
                safety_level = "–º–Ω–æ–≥–æ_–ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π"
            elif total_obstacles > 0:
                safety_level = "–µ—Å—Ç—å_–ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è"
            else:
                safety_level = "–ø—É—Ç—å_—Å–≤–æ–±–æ–¥–µ–Ω"

            return {
                "success": True,
                "safety_level": safety_level,
                "obstacles_detected": total_obstacles > 0,
                "people_detected": has_people,
                "obstacles": obstacles,
                "people": people,
                "furniture": furniture,
                "total_objects": len(detected_objects),
                "recommendation": self._get_navigation_recommendation(safety_level, has_people, total_obstacles)
            }

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏: {e}")
            return {"error": str(e)}

    def _get_navigation_recommendation(self, safety_level, has_people, obstacle_count):
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏"""
        if safety_level == "–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ":
            return "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ª—é–¥–∏ - –¥–≤–∏–≥–∞—Ç—å—Å—è –º–µ–¥–ª–µ–Ω–Ω–æ –∏ –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ"
        elif safety_level == "–º–Ω–æ–≥–æ_–ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π":
            return f"–ú–Ω–æ–≥–æ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π ({obstacle_count}) - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–∞"
        elif safety_level == "–µ—Å—Ç—å_–ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è":
            return f"–ï—Å—Ç—å –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è ({obstacle_count}) - –≤–æ–∑–º–æ–∂–µ–Ω –æ–±—ä–µ–∑–¥"
        else:
            return "–ü—É—Ç—å —Å–≤–æ–±–æ–¥–µ–Ω - –º–æ–∂–Ω–æ –¥–≤–∏–≥–∞—Ç—å—Å—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ"

    def get_scene_summary(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –æ —Ç–µ–∫—É—â–µ–π —Å—Ü–µ–Ω–µ"""
        try:
            analysis = self.analyze_current_view()
            if not analysis.get("success"):
                return {"error": analysis.get("error", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞")}

            detected_objects = analysis.get("detected_objects", [])

            # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É
            if detected_objects:
                object_counts = {}
                for obj in detected_objects:
                    class_name = obj['class']
                    object_counts[class_name] = object_counts.get(
                        class_name, 0) + 1

                summary_parts = []
                for obj_class, count in object_counts.items():
                    if count == 1:
                        summary_parts.append(obj_class)
                    else:
                        summary_parts.append(f"{obj_class} ({count})")

                summary = f"–í–∏–∂—É: {', '.join(summary_parts)}"
            else:
                summary = "–û–±—ä–µ–∫—Ç—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã"

            return {
                "success": True,
                "summary": summary,
                "object_count": len(detected_objects),
                "timestamp": analysis["timestamp"]
            }

        except Exception as e:
            return {"error": str(e)}

    def test_vision_system(self):
        """–¢–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è"""
        results = {
            "camera_test": False,
            "yolo_detection_test": False,
            "gpt4v_test": False,
            "overall_test": False,
            "details": []
        }

        try:
            # 1. –¢–µ—Å—Ç –∫–∞–º–µ—Ä—ã
            frame = self.capture_frame()
            if frame is not None:
                results["camera_test"] = True
                results["details"].append(
                    f"‚úÖ –ö–∞–º–µ—Ä–∞: –∫–∞–¥—Ä {frame.shape} –ø–æ–ª—É—á–µ–Ω")

                # 2. –¢–µ—Å—Ç YOLO –¥–µ—Ç–µ–∫—Ü–∏–∏
                if self.ai_detector:
                    detections = self.detect_objects(frame)
                    results["yolo_detection_test"] = True
                    results["details"].append(
                        f"‚úÖ YOLO: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(detections)} –æ–±—ä–µ–∫—Ç–æ–≤")
                else:
                    results["details"].append("‚ö†Ô∏è YOLO –¥–µ—Ç–µ–∫—Ç–æ—Ä –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω")

                # 3. –¢–µ—Å—Ç GPT-4V (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω API –∫–ª—é—á)
                if self.api_key:
                    try:
                        description = self.describe_scene_with_llm(
                            frame, detections if 'detections' in locals() else [])
                        if description and len(description) > 10:
                            results["gpt4v_test"] = True
                            results["details"].append(
                                f"‚úÖ GPT-4V: –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–ª—É—á–µ–Ω–æ ({len(description)} —Å–∏–º–≤–æ–ª–æ–≤)")
                        else:
                            results["details"].append(
                                "‚ùå GPT-4V: –ø—É—Å—Ç–æ–µ –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ")
                    except Exception as e:
                        results["details"].append(f"‚ùå GPT-4V: {e}")
                else:
                    results["details"].append("‚ö†Ô∏è OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

            else:
                results["details"].append("‚ùå –ö–∞–º–µ—Ä–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä")

            # –û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            total_tests = 3
            passed_tests = sum([
                results["camera_test"],
                results["yolo_detection_test"],
                results["gpt4v_test"]
            ])

            # –ú–∏–Ω–∏–º—É–º –∫–∞–º–µ—Ä–∞ + YOLO
            results["overall_test"] = passed_tests >= 2
            results["score"] = f"{passed_tests}/{total_tests}"

            logging.info(f"üß™ –¢–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã –∑—Ä–µ–Ω–∏—è: {results['score']}")
            return results

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã –∑—Ä–µ–Ω–∏—è: {e}")
            results["details"].append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            return results

    def get_status(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è"""
        return {
            "vision_analyzer": {
                "initialized": True,
                "camera_connected": self.camera is not None,
                "ai_detector_connected": self.ai_detector is not None,
                "openai_api_configured": bool(self.api_key),
                "vision_model": self.vision_model,
                "max_tokens": self.max_tokens
            }
        }
