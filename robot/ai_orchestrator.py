import json
import logging
import openai
from datetime import datetime
from pathlib import Path
from robot.ai_agent.speech_handler import SpeechHandler
from robot.ai_agent.vision_analyzer import VisionAnalyzer
from robot.ai_agent.audio_manager import AudioManager


class AIOrchestrater:
    """
    –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π AI –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä —Ä–æ–±–æ—Ç–∞
    –ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –≤—Å–µ—Ö AI –∞–≥–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ—à–µ–Ω–∏—è
    """

    def __init__(self, camera=None, robot_controller=None, ai_detector=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞
        :param camera: —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –æ–±—ä–µ–∫—Ç –∫–∞–º–µ—Ä—ã
        :param robot_controller: —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä —Ä–æ–±–æ—Ç–∞  
        :param ai_detector: —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π SimpleAIDetector
        """
        self.config = self._load_config()
        self.camera = camera
        self.robot = robot_controller
        self.ai_detector = ai_detector

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–≥–µ–Ω—Ç–æ–≤
        self.speech = None
        self.vision = None
        self.audio_manager = None

        self._initialize_agents()

        # –ò—Å—Ç–æ—Ä–∏—è –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        self.conversation_history = []
        self.current_context = {}

        logging.info("üß† AI –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _load_config(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é AI"""
        config_path = Path("data/ai_config.json")
        try:
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                logging.info("üìÑ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è AI –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return config
            else:
                # –î–µ—Ñ–æ–ª—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
                default_config = {
                    "openai_api_key": "",
                    "model": "gpt-4o-mini",
                    "speech_enabled": True,
                    "vision_enabled": True,
                    "max_conversation_length": 10,
                    "audio": {
                        "sample_rate": 16000,
                        "channels": 1,
                        "chunk_size": 1024,
                        "microphone_index": None,
                        "speaker_index": None
                    },
                    "intents": {
                        "vision_keywords": ["–≤–∏–¥–∏—à—å", "–ø–æ—Å–º–æ—Ç—Ä–∏", "—á—Ç–æ —Ç–∞–º", "–æ–ø–∏—à–∏", "—Å—Ü–µ–Ω–∞", "–∫–∞–º–µ—Ä–∞"],
                        "status_keywords": ["—Å—Ç–∞—Ç—É—Å", "—Å–æ—Å—Ç–æ—è–Ω–∏–µ", "–∫–∞–∫ –¥–µ–ª–∞", "—Ä–∞–±–æ—Ç–∞–µ—Ç", "–¥–∞—Ç—á–∏–∫–∏", "—Å–∏—Å—Ç–µ–º—ã"],
                        "action_keywords": ["–ø–æ–µ—Ö–∞–ª–∏", "–¥–≤–∏–≥–∞–π—Å—è", "–ø–æ–≤–µ—Ä–Ω–∏", "–æ—Å—Ç–∞–Ω–æ–≤–∏—Å—å", "–≤–ø–µ—Ä–µ–¥", "–Ω–∞–∑–∞–¥"],
                        "context_keywords": ["—Ä–∞—Å—Å–∫–∞–∂–∏", "–∞–Ω–∞–ª–∏–∑", "–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç", "—Å–∏—Ç—É–∞—Ü–∏—è", "–æ–±—Å—Ç–∞–Ω–æ–≤–∫–∞"]
                    }
                }

                # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
                config_path.parent.mkdir(exist_ok=True)
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(default_config, f, ensure_ascii=False, indent=2)
                logging.info("üìÑ –°–æ–∑–¥–∞–Ω–∞ –¥–µ—Ñ–æ–ª—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è AI")
                return default_config

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return {
                "openai_api_key": "",
                "speech_enabled": False,
                "vision_enabled": False
            }

    def _initialize_agents(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö AI –∞–≥–µ–Ω—Ç–æ–≤"""

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á
        api_key = self.config.get('openai_api_key')
        if not api_key:
            logging.warning("‚ö†Ô∏è OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return

        openai.api_key = api_key

        # AudioManager (–≤—Å–µ–≥–¥–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–ª—è —Ç–µ—Å—Ç–æ–≤)
        try:
            self.audio_manager = AudioManager(self.config.get('audio', {}))
            logging.info("‚úÖ AudioManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ AudioManager: {e}")

        # Speech Agent
        if self.config.get('speech_enabled', True):
            try:
                self.speech = SpeechHandler(self.config)
                self.speech.audio_manager = self.audio_manager  # –°–≤—è–∑—ã–≤–∞–µ–º —Å –∞—É–¥–∏–æ
                logging.info("‚úÖ Speech Agent –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

                # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∞—É–¥–∏–æ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
                if self.audio_manager:
                    audio_test = self.audio_manager.test_audio_system()
                    if audio_test["microphone_test"] and audio_test["speaker_test"]:
                        logging.info("‚úÖ –ê—É–¥–∏–æ —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                    else:
                        logging.warning("‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã —Å –∞—É–¥–∏–æ —Å–∏—Å—Ç–µ–º–æ–π")

            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Speech Agent: {e}")

        # Vision Agent
        if self.config.get('vision_enabled', True):
            try:
                self.vision = VisionAnalyzer(
                    config=self.config,
                    camera=self.camera,
                    ai_detector=self.ai_detector
                )
                logging.info("‚úÖ Vision Agent –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Vision Agent: {e}")

    def analyze_user_intent(self, user_text):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —á–µ—Ä–µ–∑ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ LLM"""
        if not user_text:
            return 'chat'

        user_text_lower = user_text.lower()

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–±—ã—Å—Ç—Ä–æ)
        intents = self.config.get('intents', {})

        for keyword in intents.get('vision_keywords', []):
            if keyword in user_text_lower:
                return 'vision'

        for keyword in intents.get('status_keywords', []):
            if keyword in user_text_lower:
                return 'status'

        for keyword in intents.get('action_keywords', []):
            if keyword in user_text_lower:
                return 'action'

        for keyword in intents.get('context_keywords', []):
            if keyword in user_text_lower:
                return 'context'

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM
        try:
            system_prompt = """–¢—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å –∑–∞–ø—Ä–æ—Å—ã –∫ —Ä–æ–±–æ—Ç—É –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—à—å —Ç–∏–ø –æ—Ç–≤–µ—Ç–∞.

–í–æ–∑–º–æ–∂–Ω—ã–µ —Ç–∏–ø—ã:
- "chat" - –æ–±—ã—á–Ω–∞—è –±–µ—Å–µ–¥–∞, –≤–æ–ø—Ä–æ—Å—ã, —à—É—Ç–∫–∏
- "vision" - –ø—Ä–æ—Å—å–±–∞ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –æ–ø–∏—Å–∞—Ç—å —á—Ç–æ –≤–∏–¥–∏—à—å
- "action" - –∫–æ–º–∞–Ω–¥—ã –¥–≤–∏–∂–µ–Ω–∏—è, —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º  
- "status" - –≤–æ–ø—Ä–æ—Å—ã –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Ä–æ–±–æ—Ç–∞, –¥–∞—Ç—á–∏–∫–∞—Ö
- "context" - —Å–ª–æ–∂–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Ç—Ä–µ–±—É—é—â–∏–µ –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º —Ç–∏–ø–∞."""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"–ó–∞–ø—Ä–æ—Å: '{user_text}'"}
            ]

            response = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                max_tokens=10,
                temperature=0.1
            )

            intent = response.choices[0].message.content.strip().lower()
            valid_intents = ['chat', 'vision', 'action', 'status', 'context']

            if intent in valid_intents:
                logging.info(f"üéØ LLM –æ–ø—Ä–µ–¥–µ–ª–∏–ª –Ω–∞–º–µ—Ä–µ–Ω–∏–µ: {intent}")
                return intent
            else:
                logging.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –æ—Ç LLM: {intent}")
                return 'chat'

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏–π —á–µ—Ä–µ–∑ LLM: {e}")
            return 'chat'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é

    def get_sensor_context(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –¥–∞—Ç—á–∏–∫–æ–≤ –∏ —Å–∏—Å—Ç–µ–º —Ä–æ–±–æ—Ç–∞"""
        context = {
            "timestamp": datetime.now().isoformat(),
            "robot_systems": {}
        }

        # –î–∞–Ω–Ω—ã–µ —Å —Ä–æ–±–æ—Ç–∞
        if self.robot:
            try:
                robot_status = self.robot.get_status()
                context["robot_systems"].update({
                    "status": robot_status.get("status", "unknown"),
                    "battery_voltage": robot_status.get("battery_voltage"),
                    "sensors": robot_status.get("sensors", {}),
                    "movement": robot_status.get("movement", "stopped")
                })
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ —Ä–æ–±–æ—Ç–∞: {e}")

        # –°—Ç–∞—Ç—É—Å –∫–∞–º–µ—Ä—ã
        if self.camera:
            try:
                camera_status = self.camera.get_status()
                context["camera"] = {
                    "available": camera_status.get("available", False),
                    "connected": camera_status.get("connected", False),
                    "resolution": camera_status.get("resolution", "unknown")
                }
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∫–∞–º–µ—Ä—ã: {e}")

        # –ê—É–¥–∏–æ —Å–∏—Å—Ç–µ–º–∞
        if self.audio_manager:
            try:
                devices = self.audio_manager.get_audio_devices_info()
                context["audio"] = {
                    "microphones_count": len(devices.get("microphones", [])),
                    "speakers_count": len(devices.get("speakers", [])),
                    "microphone_selected": devices.get("selected_microphone") is not None,
                    "speaker_selected": devices.get("selected_speaker") is not None
                }
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∞—É–¥–∏–æ: {e}")

        return context

    def smart_process_request(self, audio_file=None, text=None):
        """
        –ì–õ–ê–í–ù–´–ô –º–µ—Ç–æ–¥ - —É–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ª—é–±–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
        """
        try:
            # 1. –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            if audio_file:
                if not self.speech:
                    return {"error": "–ì–æ–ª–æ—Å–æ–≤–æ–π –º–æ–¥—É–ª—å –Ω–µ –∞–∫—Ç–∏–≤–µ–Ω"}

                user_text = self.speech.transcribe_audio(audio_file)
                if not user_text:
                    return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å"}

            elif text:
                user_text = text
            else:
                return {"error": "–ù–µ—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"}

            logging.info(f"üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: '{user_text}'")

            # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–º–µ—Ä–µ–Ω–∏–µ
            intent = self.analyze_user_intent(user_text)
            logging.info(f"üéØ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ: {intent}")

            # 3. –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∏—Ä—É–µ–º –Ω–∞ –Ω—É–∂–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
            if intent == 'vision':
                return self._handle_vision_request(user_text, audio_file is not None)

            elif intent == 'status':
                return self._handle_status_request(user_text, audio_file is not None)

            elif intent == 'action':
                return self._handle_action_request(user_text, audio_file is not None)

            elif intent == 'context':
                return self._handle_context_request(user_text, audio_file is not None)

            else:  # intent == 'chat'
                return self._handle_chat_request(user_text, audio_file is not None)

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ —É–º–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return {"error": str(e)}

    def _handle_vision_request(self, user_text, is_voice=False):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≥–æ —á—Ç–æ –≤–∏–¥–∏—Ç —Ä–æ–±–æ—Ç"""
        if not self.vision:
            response_text = "–ò–∑–≤–∏–Ω–∏, –º–æ–¥—É–ª—å –∑—Ä–µ–Ω–∏—è –Ω–µ –∞–∫—Ç–∏–≤–µ–Ω"
        else:
            vision_result = self.vision.analyze_current_view()
            if vision_result.get("success"):
                response_text = vision_result["description"]

                # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
                extra_data = {
                    "vision_data": vision_result,
                    "detected_objects": vision_result.get("detected_objects", [])
                }
            else:
                response_text = "–ù–µ –º–æ–≥—É –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∫–∞–º–µ—Ä—ã"
                extra_data = {}

        return self._create_response(
            user_text=user_text,
            ai_response=response_text,
            intent='vision',
            is_voice=is_voice,
            extra_data=extra_data
        )

    def _handle_status_request(self, user_text, is_voice=False):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Ä–æ–±–æ—Ç–∞"""
        try:
            context = self.get_sensor_context()

            # –§–æ—Ä–º–∏—Ä—É–µ–º —á–µ–ª–æ–≤–µ–∫–æ-—á–∏—Ç–∞–µ–º—ã–π —Å—Ç–∞—Ç—É—Å
            status_parts = []

            if context.get("robot_systems", {}).get("status"):
                status_parts.append(
                    f"–û—Å–Ω–æ–≤–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã: {context['robot_systems']['status']}")

            if context.get("robot_systems", {}).get("battery_voltage"):
                battery = context['robot_systems']['battery_voltage']
                status_parts.append(f"–ë–∞—Ç–∞—Ä–µ—è: {battery}V")

            if context.get("camera", {}).get("connected"):
                status_parts.append("–ö–∞–º–µ—Ä–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞")

            if context.get("audio", {}).get("microphone_selected"):
                status_parts.append("–ú–∏–∫—Ä–æ—Ñ–æ–Ω –∞–∫—Ç–∏–≤–µ–Ω")

            if not status_parts:
                status_parts.append("–°–∏—Å—Ç–µ–º—ã —Ä–∞–±–æ—Ç–∞—é—Ç –≤ —à—Ç–∞—Ç–Ω–æ–º —Ä–µ–∂–∏–º–µ")

            basic_status = ". ".join(status_parts)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LLM
            if self.speech:
                enhanced_prompt = f"""–°—Ç–∞—Ç—É—Å —Ä–æ–±–æ—Ç–∞: {basic_status}
                –ü–æ–¥—Ä–æ–±–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {json.dumps(context, ensure_ascii=False)}
                –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_text}
                
                –û—Ç–≤–µ—Ç—å –∫–∞–∫ —Ä–æ–±–æ—Ç-–ø–æ–º–æ—â–Ω–∏–∫, –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ –æ —Å–≤–æ–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏."""

                ai_response = self.speech.generate_response(enhanced_prompt)
            else:
                ai_response = basic_status

            return self._create_response(
                user_text=user_text,
                ai_response=ai_response,
                intent='status',
                is_voice=is_voice,
                extra_data={"context_data": context}
            )

        except Exception as e:
            return self._create_response(
                user_text=user_text,
                ai_response="–ù–µ –º–æ–≥—É –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å–∏—Å—Ç–µ–º",
                intent='status',
                is_voice=is_voice,
                extra_data={"error": str(e)}
            )

    def _handle_action_request(self, user_text, is_voice=False):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥ –¥–≤–∏–∂–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
        # TODO: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–æ–π —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º
        # –°–µ–π—á–∞—Å —Ç–æ–ª—å–∫–æ –∑–∞–≥–ª—É—à–∫–∞

        action_response = "–ü–æ–Ω—è–ª –∫–æ–º–∞–Ω–¥—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏–π –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ –ò–ò. –ò—Å–ø–æ–ª—å–∑—É–π –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è."

        # –í –±—É–¥—É—â–µ–º –∑–¥–µ—Å—å –±—É–¥–µ—Ç:
        # - –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–∞–Ω–¥—ã –¥–≤–∏–∂–µ–Ω–∏—è
        # - –í—ã–∑–æ–≤ –º–µ—Ç–æ–¥–æ–≤ robot_controller
        # - –ö–æ–Ω—Ç—Ä–æ–ª—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

        return self._create_response(
            user_text=user_text,
            ai_response=action_response,
            intent='action',
            is_voice=is_voice,
            extra_data={"planned_action": "movement_control_not_implemented"}
        )

    def _handle_context_request(self, user_text, is_voice=False):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            sensor_context = self.get_sensor_context()

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–º —á—Ç–æ –≤–∏–¥–∏—Ç —Ä–æ–±–æ—Ç
            vision_info = {}
            if self.vision:
                vision_result = self.vision.analyze_current_view()
                if vision_result.get("success"):
                    vision_info = {
                        "current_view": vision_result.get("description", ""),
                        "detected_objects": vision_result.get("detected_objects", [])
                    }

            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
            full_context = {
                "sensors_and_systems": sensor_context,
                "vision": vision_info,
                "conversation_history": self.conversation_history[-3:] if self.conversation_history else []
            }

            context_prompt = f"""–¢—ã —Ä–æ–±–æ—Ç-–ø–æ–º–æ—â–Ω–∏–∫. –í–æ—Ç –ø–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏:

{json.dumps(full_context, ensure_ascii=False, indent=2)}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_text}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Å—é –¥–æ—Å—Ç—É–ø–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –¥–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π, –Ω–æ –ø–æ–Ω—è—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –æ —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏."""

            if self.speech:
                ai_response = self.speech.generate_response(context_prompt)
            else:
                ai_response = "–î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–∏—Ç—É–∞—Ü–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º –∞–∫—Ç–∏–≤–Ω—ã–π –ò–ò –º–æ–¥—É–ª—å"

            return self._create_response(
                user_text=user_text,
                ai_response=ai_response,
                intent='context',
                is_voice=is_voice,
                extra_data={
                    "full_context": full_context,
                    "vision_data": vision_info
                }
            )

        except Exception as e:
            return self._create_response(
                user_text=user_text,
                ai_response="–ù–µ –º–æ–≥—É –ø—Ä–æ–≤–µ—Å—Ç–∏ –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–∏—Ç—É–∞—Ü–∏–∏",
                intent='context',
                is_voice=is_voice,
                extra_data={"error": str(e)}
            )

    def _handle_chat_request(self, user_text, is_voice=False):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ã—á–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        if not self.speech:
            ai_response = "–ú–æ–¥—É–ª—å –æ–±—â–µ–Ω–∏—è –Ω–µ –∞–∫—Ç–∏–≤–µ–Ω"
        else:
            ai_response = self.speech.generate_response(user_text)

        return self._create_response(
            user_text=user_text,
            ai_response=ai_response,
            intent='chat',
            is_voice=is_voice
        )

    def _create_response(self, user_text, ai_response, intent, is_voice=False, extra_data=None):
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        response = {
            "success": True,
            "user_text": user_text,
            "ai_response": ai_response,
            "intent": intent,
            "timestamp": datetime.now().isoformat(),
            "is_voice": is_voice
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if extra_data:
            response.update(extra_data)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞—É–¥–∏–æ –µ—Å–ª–∏ —ç—Ç–æ –≥–æ–ª–æ—Å–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å
        if is_voice and self.speech and self.speech.audio_manager:
            try:
                audio_file = self.speech.text_to_speech(ai_response)
                response["audio_file"] = audio_file

                # –ú–æ–∂–µ–º —Å—Ä–∞–∑—É –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏
                # speech_success = self.speech.audio_manager.play_audio(audio_file)
                # response["speech_played"] = speech_success

            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ: {e}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self._add_to_history(user_text, ai_response, intent)

        logging.info(f"ü§ñ –û—Ç–≤–µ—Ç ({intent}): '{ai_response}'")
        return response

    def voice_chat(self, recording_duration=5):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –æ–±—â–µ–Ω–∏—è —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏"""
        if not self.speech or not self.speech.audio_manager:
            return {"error": "–ê—É–¥–∏–æ —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∞–∫—Ç–∏–≤–Ω–∞"}

        try:
            logging.info("üé§ –ù–∞—á–∏–Ω–∞—é –≥–æ–ª–æ—Å–æ–≤–æ–π —á–∞—Ç...")

            # 1. –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
            audio_file = self.speech.audio_manager.record_audio(
                duration_seconds=recording_duration,
                output_file=f"data/temp_voice_{int(datetime.now().timestamp())}.wav"
            )

            if not audio_file:
                return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å –∞—É–¥–∏–æ"}

            # 2. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ —É–º–Ω—ã–π —Ä–æ—É—Ç–µ—Ä
            result = self.smart_process_request(audio_file=audio_file)

            if not result.get("success"):
                return result

            # 3. –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ –¥–∏–Ω–∞–º–∏–∫–∏
            if result.get("audio_file"):
                speech_success = self.speech.audio_manager.play_audio(
                    result["audio_file"])
                result["speech_played"] = speech_success

                if speech_success:
                    logging.info("üîä –û—Ç–≤–µ—Ç –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω —á–µ—Ä–µ–∑ –¥–∏–Ω–∞–º–∏–∫–∏")
                else:
                    logging.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ –æ—Ç–≤–µ—Ç")

            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∑–∞–ø–∏—Å–∏
            try:
                Path(audio_file).unlink(missing_ok=True)
            except:
                pass

            return result

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —á–∞—Ç–∞: {e}")
            return {"error": str(e)}

    def _add_to_history(self, user_text, ai_response, intent):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–æ–≤"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            "user_text": user_text,
            "ai_response": ai_response,
            "intent": intent
        }

        self.conversation_history.append(entry)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        max_length = self.config.get('max_conversation_length', 10)
        if len(self.conversation_history) > max_length:
            self.conversation_history = self.conversation_history[-max_length:]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
        try:
            conversations_file = Path("data/conversations.json")
            conversations_file.parent.mkdir(exist_ok=True)

            with open(conversations_file, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f,
                          ensure_ascii=False, indent=2)

        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {e}")

    # ====== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ ======

    def chat(self, audio_file=None, text=None):
        """–ü—Ä–æ—Å—Ç–æ–µ –æ–±—â–µ–Ω–∏–µ (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)"""
        return self.smart_process_request(audio_file=audio_file, text=text)

    def describe_scene(self):
        """–û–ø–∏—Å–∞—Ç—å —á—Ç–æ –≤–∏–¥–∏—Ç —Ä–æ–±–æ—Ç (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)"""
        if not self.vision:
            return {"error": "–í–∏–¥–µ–æ –º–æ–¥—É–ª—å –æ—Ç–∫–ª—é—á–µ–Ω"}
        return self.vision.analyze_current_view()

    def get_status(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å AI –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""
        return {
            "ai_orchestrator": {
                "initialized": True,
                "speech_available": self.speech is not None,
                "vision_available": self.vision is not None,
                "audio_hardware_available": (
                    self.audio_manager is not None and
                    self.audio_manager.microphone_index is not None and
                    self.audio_manager.speaker_index is not None
                ),
                "api_key_configured": bool(self.config.get('openai_api_key')),
                "conversation_entries": len(self.conversation_history),
                "config": {
                    "speech_enabled": self.config.get('speech_enabled', False),
                    "vision_enabled": self.config.get('vision_enabled', False),
                    "model": self.config.get('model', 'not set')
                }
            }
        }

    def update_config(self, new_config):
        """–û–±–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–≥–µ–Ω—Ç–æ–≤"""
        try:
            self.config.update(new_config)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            config_path = Path("data/ai_config.json")
            config_path.parent.mkdir(exist_ok=True)

            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, ensure_ascii=False, indent=2)

            # –ü–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–≥–µ–Ω—Ç–æ–≤ —Å –Ω–æ–≤–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
            self._initialize_agents()

            logging.info(
                "‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è AI –æ–±–Ω–æ–≤–ª–µ–Ω–∞ –∏ –∞–≥–µ–Ω—Ç—ã –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
            return {"success": True, "message": "–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞"}

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return {"error": str(e)}

    def get_conversation_history(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–æ–≤"""
        return {
            "history": self.conversation_history,
            "total_entries": len(self.conversation_history)
        }

    def clear_conversation_history(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–æ–≤"""
        self.conversation_history = []
        try:
            conversations_file = Path("data/conversations.json")
            if conversations_file.exists():
                conversations_file.unlink()
            return {"success": True, "message": "–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–æ–≤ –æ—á–∏—â–µ–Ω–∞"}
        except Exception as e:
            return {"error": str(e)}
