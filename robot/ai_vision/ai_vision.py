# robot/ai_vision.py
"""
AI Vision Module - –ë–∞–∑–æ–≤–æ–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –¥–ª—è —Ä–æ–±–æ—Ç–∞
–≠—Ç–∞–ø 1: –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –ª–∏—Ü, –¥–≤–∏–∂–µ–Ω–∏—è
"""

from __future__ import annotations
import cv2
import numpy as np
import threading
import time
import logging
from typing import List, Dict, Optional, Callable, Tuple
from dataclasses import dataclass
from pathlib import Path

logger = logging.getLogger(__name__)


@dataclass
class DetectedObject:
    """–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç"""
    class_name: str
    confidence: float
    bbox: Tuple[int, int, int, int]  # x, y, w, h
    center: Tuple[int, int]
    area: float
    timestamp: float


@dataclass
class VisionState:
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è"""
    objects: List[DetectedObject]
    faces: List[Dict]
    motion_detected: bool
    scene_description: str
    processing_fps: float
    last_update: float


class AIVisionProcessor:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è"""

    def __init__(self, camera=None):
        self.camera = camera
        self.state = VisionState(
            objects=[],
            faces=[],
            motion_detected=False,
            scene_description="",
            processing_fps=0.0,
            last_update=0.0
        )

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
        self._init_detectors()

        # –ü–æ—Ç–æ–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self._processing_thread: Optional[threading.Thread] = None
        self._stop_event = threading.Event()
        self._lock = threading.RLock()

        # –ö–æ–ª–±—ç–∫–∏ –¥–ª—è —Å–æ–±—ã—Ç–∏–π
        self._callbacks: Dict[str, List[Callable]] = {
            'object_detected': [],
            'face_detected': [],
            'motion_detected': [],
            'person_detected': []
        }

        # –ë—É—Ñ–µ—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–≤–∏–∂–µ–Ω–∏—è
        self._motion_history = []
        self._background_subtractor = cv2.createBackgroundSubtractorMOG2(
            detectShadows=True)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ FPS
        self._frame_times = []

    def _init_detectors(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤"""
        logger.info("üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤...")

        try:
            # –î–µ—Ç–µ–∫—Ç–æ—Ä –ª–∏—Ü Haar Cascade
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            logger.info("‚úÖ Haar Cascade –¥–µ—Ç–µ–∫—Ç–æ—Ä –ª–∏—Ü –∑–∞–≥—Ä—É–∂–µ–Ω")

            # –î–µ—Ç–µ–∫—Ç–æ—Ä —Ç–µ–ª–∞
            body_cascade_path = cv2.data.haarcascades + 'haarcascade_fullbody.xml'
            self.body_cascade = cv2.CascadeClassifier(body_cascade_path)
            logger.info("‚úÖ Haar Cascade –¥–µ—Ç–µ–∫—Ç–æ—Ä —Ç–µ–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Haar –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤: {e}")
            self.face_cascade = None
            self.body_cascade = None

        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å YOLO (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        self._init_yolo()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
        self._init_advanced_detectors()

    def _init_yolo(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è YOLO –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞"""
        self.yolo_net = None
        self.yolo_classes = []

        try:
            # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º YOLO (—Å–æ–∑–¥–∞–¥–∏–º —Å–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∑–∫–∏)
            weights_path = "models/yolo/yolov4-tiny.weights"
            config_path = "models/yolo/yolov4-tiny.cfg"
            names_path = "models/yolo/coco.names"

            if Path(weights_path).exists() and Path(config_path).exists():
                self.yolo_net = cv2.dnn.readNet(weights_path, config_path)

                if Path(names_path).exists():
                    with open(names_path, 'r') as f:
                        self.yolo_classes = [line.strip()
                                             for line in f.readlines()]

                logger.info(
                    f"‚úÖ YOLO –∑–∞–≥—Ä—É–∂–µ–Ω —Å {len(self.yolo_classes)} –∫–ª–∞—Å—Å–∞–º–∏")
            else:
                logger.info(
                    "‚ö†Ô∏è YOLO —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Haar –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ YOLO: {e}")

    def _init_advanced_detectors(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤"""
        # –î–µ—Ç–µ–∫—Ç–æ—Ä —É–≥–ª–æ–≤/–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
        self.corner_detector = cv2.goodFeaturesToTrack

        # –î–µ—Ç–µ–∫—Ç–æ—Ä –∫–æ–Ω—Ç—É—Ä–æ–≤
        self.contour_detector = cv2.findContours

        # –û–ø—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Ç–æ–∫ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è
        self.optical_flow = cv2.calcOpticalFlowPyrLK

        logger.info("‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")

    def start_processing(self) -> bool:
        """–ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
        if self._processing_thread and self._processing_thread.is_alive():
            logger.warning("–û–±—Ä–∞–±–æ—Ç–∫–∞ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–∞")
            return True

        if not self.camera:
            logger.error("–ö–∞–º–µ—Ä–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return False

        self._stop_event.clear()
        self._processing_thread = threading.Thread(
            target=self._processing_loop, daemon=True)
        self._processing_thread.start()

        logger.info("üöÄ AI Vision –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—É—â–µ–Ω–∞")
        return True

    def stop_processing(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self._stop_event.set()
        if self._processing_thread:
            self._processing_thread.join(timeout=3.0)
        logger.info("‚èπÔ∏è AI Vision –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

    def _processing_loop(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        logger.info("üîÑ –ó–∞–ø—É—â–µ–Ω —Ü–∏–∫–ª AI –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        while not self._stop_event.is_set():
            start_time = time.time()

            try:
                # –ü–æ–ª—É—á–∞–µ–º –∫–∞–¥—Ä –æ—Ç –∫–∞–º–µ—Ä—ã
                frame = self._get_frame()
                if frame is None:
                    time.sleep(0.1)
                    continue

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–¥—Ä
                self._process_frame(frame)

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ FPS
                self._update_fps_stats(start_time)

                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º FPS –æ–±—Ä–∞–±–æ—Ç–∫–∏
                time.sleep(max(0, 1/10 - (time.time() - start_time)))  # 10 FPS

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                time.sleep(1.0)

        logger.info("üîö –¶–∏–∫–ª AI –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω")

    def _get_frame(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–∞–¥—Ä–∞ –æ—Ç –∫–∞–º–µ—Ä—ã"""
        if not self.camera or not hasattr(self.camera, '_current_frame'):
            return None

        with self.camera._frame_lock:
            if self.camera._current_frame is not None:
                return self.camera._current_frame.copy()
        return None

    def _process_frame(self, frame):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∫–∞–¥—Ä–∞"""
        with self._lock:
            # –°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è
            self.state.objects = []
            self.state.faces = []
            self.state.motion_detected = False

            # –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü
            faces = self._detect_faces(frame)
            self.state.faces = faces

            # –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
            objects = self._detect_objects(frame)
            self.state.objects = objects

            # –î–µ—Ç–µ–∫—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏—è
            motion = self._detect_motion(frame)
            self.state.motion_detected = motion

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è —Å—Ü–µ–Ω—ã
            self.state.scene_description = self._generate_scene_description()

            self.state.last_update = time.time()

            # –í—ã–∑–æ–≤ –∫–æ–ª–±—ç–∫–æ–≤
            self._trigger_callbacks(frame)

    def _detect_faces(self, frame) -> List[Dict]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü"""
        if not self.face_cascade:
            return []

        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = self.face_cascade.detectMultiScale(
                gray,
                scaleFactor=1.1,
                minNeighbors=5,
                minSize=(30, 30)
            )

            detected_faces = []
            for (x, y, w, h) in faces:
                face = {
                    'bbox': (x, y, w, h),
                    'center': (x + w//2, y + h//2),
                    'area': w * h,
                    'confidence': 0.8,  # Haar –Ω–µ –¥–∞–µ—Ç —Ç–æ—á–Ω—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                    'timestamp': time.time()
                }
                detected_faces.append(face)

            if detected_faces:
                logger.debug(f"üë§ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ª–∏—Ü: {len(detected_faces)}")

            return detected_faces

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª–∏—Ü: {e}")
            return []

    def _detect_objects(self, frame) -> List[DetectedObject]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤"""
        detected_objects = []

        # YOLO –¥–µ—Ç–µ–∫—Ü–∏—è (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
        if self.yolo_net is not None:
            yolo_objects = self._detect_with_yolo(frame)
            detected_objects.extend(yolo_objects)

        # –î–µ—Ç–µ–∫—Ü–∏—è –ª—é–¥–µ–π —á–µ—Ä–µ–∑ Haar Cascade
        if self.body_cascade:
            people = self._detect_people_haar(frame)
            detected_objects.extend(people)

        return detected_objects

    def _detect_with_yolo(self, frame) -> List[DetectedObject]:
        """–î–µ—Ç–µ–∫—Ü–∏—è —Å –ø–æ–º–æ—â—å—é YOLO"""
        if not self.yolo_net:
            return []

        try:
            height, width = frame.shape[:2]

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–ª–æ–±–∞ –¥–ª—è YOLO
            blob = cv2.dnn.blobFromImage(
                frame, 1/255.0, (416, 416),
                swapRB=True, crop=False
            )
            self.yolo_net.setInput(blob)

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–π
            layer_outputs = self.yolo_net.forward(
                self.yolo_net.getUnconnectedOutLayersNames()
            )

            objects = []
            for output in layer_outputs:
                for detection in output:
                    scores = detection[5:]
                    class_id = np.argmax(scores)
                    confidence = scores[class_id]

                    if confidence > 0.5:  # –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                        center_x = int(detection[0] * width)
                        center_y = int(detection[1] * height)
                        w = int(detection[2] * width)
                        h = int(detection[3] * height)

                        x = int(center_x - w/2)
                        y = int(center_y - h/2)

                        obj = DetectedObject(
                            class_name=self.yolo_classes[class_id] if class_id < len(
                                self.yolo_classes) else f"class_{class_id}",
                            confidence=float(confidence),
                            bbox=(x, y, w, h),
                            center=(center_x, center_y),
                            area=w * h,
                            timestamp=time.time()
                        )
                        objects.append(obj)

            return objects

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ YOLO –¥–µ—Ç–µ–∫—Ü–∏–∏: {e}")
            return []

    def _detect_people_haar(self, frame) -> List[DetectedObject]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –ª—é–¥–µ–π —á–µ—Ä–µ–∑ Haar Cascade"""
        if not self.body_cascade:
            return []

        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            people = self.body_cascade.detectMultiScale(
                gray,
                scaleFactor=1.1,
                minNeighbors=3,
                minSize=(50, 100)
            )

            detected_people = []
            for (x, y, w, h) in people:
                person = DetectedObject(
                    class_name="person",
                    confidence=0.7,
                    bbox=(x, y, w, h),
                    center=(x + w//2, y + h//2),
                    area=w * h,
                    timestamp=time.time()
                )
                detected_people.append(person)

            return detected_people

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª—é–¥–µ–π: {e}")
            return []

    def _detect_motion(self, frame) -> bool:
        """–î–µ—Ç–µ–∫—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏—è"""
        try:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º Background Subtractor
            fg_mask = self._background_subtractor.apply(frame)

            # –£–±–∏—Ä–∞–µ–º —à—É–º
            kernel = np.ones((5, 5), np.uint8)
            fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)

            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∏–∫—Å–µ–ª–∏ –ø–µ—Ä–µ–¥–Ω–µ–≥–æ –ø–ª–∞–Ω–∞
            motion_pixels = cv2.countNonZero(fg_mask)
            total_pixels = frame.shape[0] * frame.shape[1]
            motion_ratio = motion_pixels / total_pixels

            # –ü–æ—Ä–æ–≥ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è
            motion_detected = motion_ratio > 0.01  # 1% –∫–∞–¥—Ä–∞

            if motion_detected:
                logger.debug(f"üèÉ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –¥–≤–∏–∂–µ–Ω–∏–µ: {motion_ratio:.3f}")

            return motion_detected

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è: {e}")
            return False

    def _generate_scene_description(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è —Å—Ü–µ–Ω—ã"""
        description_parts = []

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Ü
        if self.state.faces:
            face_count = len(self.state.faces)
            if face_count == 1:
                description_parts.append("—á–µ–ª–æ–≤–µ–∫")
            else:
                description_parts.append(f"{face_count} —á–µ–ª–æ–≤–µ–∫–∞")

        # –û–±—ä–µ–∫—Ç—ã
        if self.state.objects:
            object_names = [obj.class_name for obj in self.state.objects]
            unique_objects = {}
            for name in object_names:
                unique_objects[name] = unique_objects.get(name, 0) + 1

            for obj_name, count in unique_objects.items():
                if count == 1:
                    description_parts.append(obj_name)
                else:
                    description_parts.append(f"{count} {obj_name}")

        # –î–≤–∏–∂–µ–Ω–∏–µ
        if self.state.motion_detected:
            description_parts.append("–¥–≤–∏–∂–µ–Ω–∏–µ")

        if not description_parts:
            return "–ø—É—Å—Ç–∞—è —Å—Ü–µ–Ω–∞"

        return "–í–∏–∂—É: " + ", ".join(description_parts)

    def _update_fps_stats(self, start_time):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ FPS"""
        processing_time = time.time() - start_time
        self._frame_times.append(processing_time)

        if len(self._frame_times) > 30:
            self._frame_times.pop(0)

        if len(self._frame_times) > 1:
            avg_time = sum(self._frame_times) / len(self._frame_times)
            self.state.processing_fps = 1.0 / max(avg_time, 0.001)

    def _trigger_callbacks(self, frame):
        """–í—ã–∑–æ–≤ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–ª–±—ç–∫–æ–≤"""
        try:
            # –ö–æ–ª–±—ç–∫–∏ –¥–ª—è –ª–∏—Ü
            if self.state.faces and 'face_detected' in self._callbacks:
                for callback in self._callbacks['face_detected']:
                    callback(self.state.faces, frame)

            # –ö–æ–ª–±—ç–∫–∏ –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤
            if self.state.objects:
                for callback in self._callbacks.get('object_detected', []):
                    callback(self.state.objects, frame)

                # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∫–æ–ª–±—ç–∫–∏ –¥–ª—è –ª—é–¥–µ–π
                people = [
                    obj for obj in self.state.objects if obj.class_name == 'person']
                if people:
                    for callback in self._callbacks.get('person_detected', []):
                        callback(people, frame)

            # –ö–æ–ª–±—ç–∫–∏ –¥–ª—è –¥–≤–∏–∂–µ–Ω–∏—è
            if self.state.motion_detected:
                for callback in self._callbacks.get('motion_detected', []):
                    callback(frame)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ –∫–æ–ª–±—ç–∫–∞—Ö: {e}")

    def add_callback(self, event_type: str, callback: Callable):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–±—ç–∫–∞ –¥–ª—è —Å–æ–±—ã—Ç–∏—è"""
        if event_type not in self._callbacks:
            self._callbacks[event_type] = []
        self._callbacks[event_type].append(callback)
        logger.info(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω –∫–æ–ª–±—ç–∫ –¥–ª—è —Å–æ–±—ã—Ç–∏—è: {event_type}")

    def remove_callback(self, event_type: str, callback: Callable):
        """–£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–ª–±—ç–∫–∞"""
        if event_type in self._callbacks and callback in self._callbacks[event_type]:
            self._callbacks[event_type].remove(callback)
            logger.info(f"‚ûñ –£–¥–∞–ª–µ–Ω –∫–æ–ª–±—ç–∫ –¥–ª—è —Å–æ–±—ã—Ç–∏—è: {event_type}")

    def get_annotated_frame(self, frame) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–∞–¥—Ä–∞ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏"""
        annotated = frame.copy()

        with self._lock:
            # –†–∏—Å—É–µ–º –ª–∏—Ü–∞
            for face in self.state.faces:
                x, y, w, h = face['bbox']
                cv2.rectangle(annotated, (x, y), (x+w, y+h), (0, 255, 0), 2)
                cv2.putText(annotated, 'Face', (x, y-10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

            # –†–∏—Å—É–µ–º –æ–±—ä–µ–∫—Ç—ã
            for obj in self.state.objects:
                x, y, w, h = obj.bbox
                cv2.rectangle(annotated, (x, y), (x+w, y+h), (255, 0, 0), 2)
                label = f"{obj.class_name}: {obj.confidence:.2f}"
                cv2.putText(annotated, label, (x, y-10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)

            # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –¥–≤–∏–∂–µ–Ω–∏—è
            if self.state.motion_detected:
                cv2.putText(annotated, 'MOTION DETECTED', (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

            # –û–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã
            cv2.putText(annotated, self.state.scene_description, (10, annotated.shape[0] - 20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)

        return annotated

    def get_state(self) -> VisionState:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        with self._lock:
            return self.state

    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —Ä–æ–±–æ—Ç–∞
    def is_person_in_front(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –≤–ø–µ—Ä–µ–¥–∏"""
        with self._lock:
            for obj in self.state.objects:
                if obj.class_name == 'person':
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —á–µ–ª–æ–≤–µ–∫ –≤ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–∏ –∫–∞–¥—Ä–∞
                    center_x = obj.center[0]
                    frame_center = 320  # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º 640px —à–∏—Ä–∏–Ω—É
                    if abs(center_x - frame_center) < 100:  # –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 100px –æ—Ç —Ü–µ–Ω—Ç—Ä–∞
                        return True
            return False

    def get_closest_person(self) -> Optional[DetectedObject]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –±–ª–∏–∂–∞–π—à–µ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ (–ø–æ —Ä–∞–∑–º–µ—Ä—É –æ–±–ª–∞—Å—Ç–∏)"""
        with self._lock:
            people = [
                obj for obj in self.state.objects if obj.class_name == 'person']
            if not people:
                return None
            return max(people, key=lambda p: p.area)

    def count_detected_objects(self) -> Dict[str, int]:
        """–ü–æ–¥—Å—á–µ—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ —Ç–∏–ø–∞–º"""
        with self._lock:
            counts = {}
            for obj in self.state.objects:
                counts[obj.class_name] = counts.get(obj.class_name, 0) + 1
            return counts
